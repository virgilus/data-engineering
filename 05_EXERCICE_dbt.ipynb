{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b228d6da",
   "metadata": {},
   "source": [
    "# dbt exercices\n",
    "\n",
    "We're going to use a fake database named `sciencestreaming` that contains data about a streaming platform for science documentaries.\n",
    "\n",
    "## Running the sciencestreaming database\n",
    "\n",
    "### Option 1: Using Docker (recommended)\n",
    "\n",
    "1. Make sure you have [Docker installed](https://docs.docker.com/get-docker/).\n",
    "2. Navigate to the `data-engineering/sciencestreaming` directory in your terminal.\n",
    "3. Run the compose.yaml file with the following command:\n",
    "\n",
    "   ```bash\n",
    "   docker compose up -d\n",
    "   ```\n",
    "4. The MySQL database will be accessible at `localhost:3306` with the following credentials:\n",
    "   - Username: `root`\n",
    "   - Password: `azerty`\n",
    "   - Database: `sciencestreaming`\n",
    "   - You can connect to it using any MySQL client or tool.\n",
    "   - To stop the database, run:\n",
    "   \n",
    "   ```bash\n",
    "   docker compose down\n",
    "   ```\n",
    "### Option 2: Using XAMPP\n",
    "\n",
    "1. Make sure you have [XAMPP installed](https://www.apachefriends.org/index.html).\n",
    "2. Start the MySQL service from the XAMPP control panel.\n",
    "3. Import the `sciencestreaming.sql.gz` file located in the `data/compressed` directory into your MySQL server using phpMyAdmin or the MySQL command line.\n",
    "\n",
    "## The database schema\n",
    "\n",
    "If we type the following SQL query:\n",
    "\n",
    "```sql\n",
    "SELECT table_name, column_name, data_type, column_key \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = 'sciencestreaming' \n",
    "ORDER BY table_name, ordinal_position;\n",
    "```\n",
    "\n",
    "We get the following result:\n",
    "\n",
    "| table_name | column_name | data_type |\n",
    "|---|---|---|\n",
    "| course | course_id | bigint |\n",
    "| course | course_title | text |\n",
    "| course | subject_name | text |\n",
    "| schedule | schedule_id | bigint |\n",
    "| schedule | start_time | datetime |\n",
    "| schedule | end_time | datetime |\n",
    "| schedule | duration_mn | bigint |\n",
    "| schedule | course_id | bigint |\n",
    "| schedule | teacher_id | bigint |\n",
    "| sub | sub_id | bigint |\n",
    "| sub | user_id | bigint |\n",
    "| sub | start_date | datetime |\n",
    "| sub | end_date | datetime |\n",
    "| sub | price | text |\n",
    "| teacher | teacher_id | bigint |\n",
    "| teacher | first_name | text |\n",
    "| teacher | last_name | text |\n",
    "| teacher | birthdate | date |\n",
    "| user | user_id | bigint |\n",
    "| user | gender | text |\n",
    "| user | birthdate | datetime |\n",
    "| user | created_at | datetime |\n",
    "| user | department_code | text |\n",
    "| user | first_name | text |\n",
    "| user | last_name | text |\n",
    "| view | view_id | bigint |\n",
    "| view | user_id | bigint |\n",
    "| view | viewed_at | datetime |\n",
    "| view | view_type | text |\n",
    "| view | schedule_id | bigint |\n",
    "| view | watched_seconds | bigint |\n",
    "\n",
    "## A quick look at all the tables\n",
    "\n",
    "### The table `course`\n",
    "\n",
    "| course_id | course_title | subject_name |\n",
    "|---|---|---|\n",
    "| 1 | Algorithmique | Informatique théorique |\n",
    "| 2 | Informatique quantique | Informatique théorique |\n",
    "| 3 | Intelligence artificielle | Informatique théorique |\n",
    "| 4 | Méthodes formelles | Informatique théorique |\n",
    "| 5 | Structures de données | Informatique théorique |\n",
    "\n",
    "### The table `schedule`\n",
    "\n",
    "| schedule_id | start_time | end_time | duration_mn | course_id | teacher_id |\n",
    "|---|---|---|---|---|---|\n",
    "| 53 | 2020-12-15 18:30:00 | 2020-12-15 19:15:00 | 45 | 87 | 33 |\n",
    "| 7 | 2020-07-28 17:30:00 | 2020-07-28 18:15:00 | 45 | 7 | 17 |\n",
    "| 92 | 2020-09-15 20:30:00 | 2020-09-15 21:15:00 | 45 | 16 | 15 |\n",
    "| 189 | 2020-07-29 16:00:00 | 2020-07-29 16:30:00 | 30 | 22 | 29 |\n",
    "| 41 | 2020-09-22 18:30:00 | 2020-09-22 19:15:00 | 45 | 15 | 46 |\n",
    "\n",
    "### The table `sub`\n",
    "\n",
    "| sub_id | user_id | start_date | end_date | price |\n",
    "|---|---|---|---|---|\n",
    "| 702300 | 1 | 2020-08-05 00:00:00 | 2021-08-05 00:00:00 | 0 |\n",
    "| 702313 | 2 | 2020-08-05 00:00:00 | 2021-08-05 00:00:00 | 0 |\n",
    "| 702256 | 3 | 2020-08-05 00:00:00 | 2020-09-05 00:00:00 | 0 |\n",
    "| 703146 | 1024 | 2020-08-05 00:00:00 | 2020-09-05 00:00:00 | 0 |\n",
    "| 702308 | 5 | 2020-08-05 00:00:00 | 2021-08-05 00:00:00 | 0 |\n",
    "\n",
    "### The table `teacher`\n",
    "\n",
    "| teacher_id | first_name | last_name | birthdate |\n",
    "|---|---|---|---|\n",
    "| 1 | Nadia | LEHUE | 1970-12-06 |\n",
    "| 2 | Odette | BENTIL | 1983-06-10 |\n",
    "| 3 | Rolande | ZERLAUDT | 1962-01-31 |\n",
    "| 4 | Olivier | VIDNAR | 1959-11-15 |\n",
    "| 5 | Timothée | BARRASS | 1959-09-19 |\n",
    "\n",
    "### The table `user`\n",
    "\n",
    "| user_id | gender | birthdate | created_at | department_code | first_name | last_name |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 1 | H | 1988-10-27 00:00:00 | 2020-08-05 15:28:30 | 93 | Adolphe | CARMOUL |\n",
    "| 2 | F | 1990-09-11 00:00:00 | 2020-08-05 15:28:36 | 77 | Adélaïde | BAX MARIE |\n",
    "| 3 | H | 1989-09-18 00:00:00 | 2020-08-03 16:33:32 | 59 | Jean-marie | TAQUOIS |\n",
    "| 4 | H | 1990-01-19 00:00:00 | 2020-08-05 15:28:34 | 94 | Nazaire | JARRAND MARTIN |\n",
    "| 5 | H | 1986-06-18 00:00:00 | 2020-08-05 15:28:34 | 94 | Valère | CALAMATIANOS |\n",
    "\n",
    "### The table `view`\n",
    "\n",
    "| view_id | user_id | viewed_at | view_type | schedule_id | watched_seconds |\n",
    "|---|---|---|---|---|---|\n",
    "| 1 | 6209 | 2020-10-31 11:43:30 | Live | 771 | 60 |\n",
    "| 2 | 31170 | 2021-02-22 10:59:58 | Live | 3191 | 1719 |\n",
    "| 3 | 27611 | 2020-11-29 18:46:43 | Replay | 3345 | 39 |\n",
    "| 4 | 6209 | 2021-02-08 21:04:15 | Live | 4737 | 2700 |\n",
    "| 5 | 10656 | 2020-11-06 09:53:50 | Replay | 3053 | 1800 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b219105",
   "metadata": {},
   "source": [
    "\n",
    "## dbt install\n",
    "\n",
    "It's a very good practice to install dbt in a virtual environment, so you can avoid dependency conflicts with other Python projects. So first create a new virtual environment either with conda or venv and install dbt with the following command:\n",
    "\n",
    "```bash\n",
    "conda create -n dbt_env python==3.11 -y\n",
    "conda activate dbt_env\n",
    "```\n",
    "\n",
    "```bash\n",
    "python -m pip install dbt-core dbt-mysql\n",
    "```\n",
    "\n",
    "The use of \"python -m pip\" ensures that you are using the pip associated with your virtual environment.\n",
    "\n",
    "dbt can be tricky to install depending on your OS and Python version. Try not to use the latest Python version if you encounter issues. Also you might need to downgrade some librairies (for example : pip install \"protobuf<5\")\n",
    "\n",
    "## Create a dbt project\n",
    "\n",
    "1. Open your terminal and navigate to the directory where you want to create your dbt project.\n",
    "2. Run the following command to create a new dbt project named `sciencestreaming`:\n",
    "   ```bash\n",
    "   dbt init sciencestreaming\n",
    "   ```\n",
    "3. Navigate into the newly created project directory:\n",
    "   ```bash\n",
    "   cd sciencestreaming\n",
    "   ```\n",
    "4. Configure your `profiles.yml` file to connect to the `sciencestreaming` database. You can find the `profiles.yml` file in the `~/.dbt/` directory. Here is an example configuration for a MySQL database:\n",
    "\n",
    "   ```yaml\n",
    "    sciencestreaming:\n",
    "    target: dev\n",
    "    outputs:\n",
    "        dev:\n",
    "            type: mysql\n",
    "            server: localhost\n",
    "            port: 3306\n",
    "            database: sciencestreaming\n",
    "            schema: sciencestreaming\n",
    "            username: root\n",
    "            password: azerty\n",
    "            driver: MySQL ODBC 8.0 ANSI Driver\n",
    "        prod:\n",
    "            type: mysql\n",
    "            server: localhost\n",
    "            port: 3306\n",
    "            database: sciencestreaming\n",
    "            schema: sciencestreaming\n",
    "            username: root\n",
    "            password: azerty\n",
    "            driver: MySQL ODBC 8.0 ANSI Driver\n",
    "   ```\n",
    "5. Test the connection by running:\n",
    "   ```bash\n",
    "   dbt debug\n",
    "   ```\n",
    "6. You are now ready to start building your dbt models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33399f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## dbt architecture\n",
    "\n",
    "After you've created your dbt project, you'll notice a specific directory structure. Here's a brief overview of the files and folders you'll find in a typical dbt project:\n",
    "- **dbt_project.yml**: The main configuration file for your dbt project. It defines settings like the project name, version, and model configurations.\n",
    "- **profiles.yml**: This file is not located in the project directory but in the `~/.dbt/` directory. It contains the connection details for your data warehouse.\n",
    "  \n",
    "- **analysis**: SQL files that contain analysis queries. They are stored in the `analysis` directory.\n",
    "- **logs**: Directory where dbt stores log files.\n",
    "- **macros**: SQL files that define reusable SQL snippets or functions. They are stored in the `macros` directory.\n",
    "- **models**: SQL files that define transformations on your raw data. They are stored in the `models` directory.\n",
    "- **seeds**: CSV files that can be loaded into your data warehouse as tables. They are stored in the `seeds` directory.\n",
    "- **snapshots**: SQL files that define how to capture changes in your source data over time. They are stored in the `snapshots` directory.\n",
    "- **tests**: SQL files that define tests to ensure data quality. They are stored in the `tests` directory.\n",
    "\n",
    "## Let's create a new dbt model\n",
    "\n",
    "As an example, let's create a simple dbt model that selects all users from the `user` table that have a paid subscription.\n",
    "\n",
    "1. In the `models` directory of your dbt project, create a new SQL file named `paid_users.sql`. In order to check if a user has a paid subscription we need, for each user, to check all subscriptions and see if at least one of them has a price greater than 0. In sql that would give :\n",
    "\n",
    "```sql\n",
    "WITH paid_subs AS (\n",
    "    SELECT DISTINCT user_id\n",
    "    FROM sub\n",
    "    WHERE price > 0\n",
    ")\n",
    "SELECT u.*\n",
    "FROM user u\n",
    "JOIN paid_subs ps ON u.user_id = ps.user_id\n",
    "```\n",
    "\n",
    "\n",
    "Try the code inside your SQL client. If it works, we're going to modify  it so it includes reference and then save it inside a `paid_users.sql` file.\n",
    "\n",
    "```sql\n",
    "WITH paid_subs AS (\n",
    "    SELECT DISTINCT user_id\n",
    "    FROM {{ source('sciencestreaming', 'sub') }}\n",
    "    WHERE price > 0\n",
    ")\n",
    "SELECT u.*\n",
    "FROM {{ source('sciencestreaming', 'user') }} u\n",
    "JOIN paid_subs ps ON u.user_id = ps.user_id\n",
    "```\n",
    "Note : Make sure to **NOT** add a semicolon at the end of the query, as dbt might throw an error.\n",
    "\n",
    "## Sources\n",
    "\n",
    "In dbt, sources are used to define and document the raw data tables in your data warehouse. They help you manage dependencies and ensure that your models are built on top of the correct source tables.\n",
    "\n",
    "In the same folder, create a new file named `sources.yaml` to define the source tables. Add the following content to the `sources.yaml` file:\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: sciencestreaming\n",
    "    tables:\n",
    "      - name: sub\n",
    "      - name: user\n",
    "      - name: course\n",
    "      - name: schedule\n",
    "      - name: teacher\n",
    "      - name: view\n",
    "```\n",
    "\n",
    "Thans to this file, dbt will know that the tables `sub` and `user` are coming from the `sciencestreaming` database.\n",
    "\n",
    "Now run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "dbt run\n",
    "```\n",
    "\n",
    "This command will execute the SQL code in your `paid_users.sql` model and create a new table or view in your data warehouse based on the logic defined in the model.\n",
    "\n",
    "By default, dbt creates views for models. If you want to create tables instead, you can configure this in the `dbt_project.yml` file by adding the following lines:\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  sciencestreaming:\n",
    "    +materialized: table\n",
    "```\n",
    "\n",
    "Why using a \"+\" before \"materialized\" ? The \"+\" sign indicates that this configuration should be applied to all models within the `sciencestreaming` project. It acts as a prefix, meaning that any model defined in this project will inherit this setting unless explicitly overridden in the model itself.\n",
    "\n",
    "But if you want to create only the `paid_users` model as a table, you can add the following line at the top of your `paid_users.sql` file:\n",
    "\n",
    "```sql\n",
    "{{ config(materialized='table') }}\n",
    "``` \n",
    "\n",
    "It will override the project-level configuration for this specific model.\n",
    "\n",
    "## Useful dbt commands\n",
    "\n",
    "\n",
    "1. ```dbt clean```: This command removes all files in the `target` and `dbt_modules` directories. It's useful for cleaning up your project before a fresh build.\n",
    "\n",
    "2. ```dbt deps```: This command installs the dependencies specified in your `packages.yml` file. If you have any dbt packages that your project relies on, this command will download and install them.\n",
    "\n",
    "3. ```dbt snapshot```: This command runs the snapshot definitions in your project. Snapshots are used to capture the state of your source data at specific points in time.\n",
    "\n",
    "4. ```dbt run```: This command runs the models in your project, creating tables or views in your data warehouse based on the SQL code in your model files.\n",
    "\n",
    "5. ```dbt test```: This command runs the tests defined in your project to ensure data quality and integrity.\n",
    "\n",
    "6. ```dbt docs generate```: This command generates documentation for your dbt project, including information about models, sources, and tests.\n",
    "\n",
    "7. ```dbt docs serve```: This command starts a local web server to serve the documentation generated by `dbt docs generate`. You can view the documentation in your web browser.\n",
    "\n",
    "In order to test all these commands you can try doing this tutorial :\n",
    "\n",
    "https://www.startdataengineering.com/post/dbt-data-build-tool-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
